{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\t** Continuous or quantitative:** variables can take any positive or negative numerical value between a large range. Retail sales amount, insurance claims amounts are examples for continuous variables that can take any number within large ranges. These types of variables are also generally known as numerical variables.\n",
    "\n",
    "2.\t** Discrete or qualitative:** variables can take only particular values: retail store location area, state, city are examples for discrete variables as it can take only one particular value for a store (here store is our object). These types of variables are also known as categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scales of Measurement\n",
    "In general, variables can be measured on **four different scales**. \n",
    "1. **Mean, median, and mode** are the way to understand the central tendency, that is, the middle point of data distribution. \n",
    "2. **Standard deviation, variance, and range** are the most commonly used dispersion measures used to understand the spread of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nominal Scale of Measurement\n",
    "Data are measured at the nominal level when each case is classified into one of a number of discrete categories. This is also called **categorical**, that is, used only for classification. As mean is not meaningful, all that we can do is to **count the number of occurrences of each type and compute the proportion (number of occurrences of each type / total occurrences).**\n",
    "<br>\n",
    "**Nomial scale examples**<br>\n",
    "**Color:** Red, Green, Yellow, etc.<br>\n",
    "**Gender:** Female, Male<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal Scale of Measurement\n",
    "Data are measured on an ordinal scale if the categories imply order. The difference between ranks is consistent in direction and authority, but not magnitude. <br>\n",
    "**Ordinal scale example**<br>\n",
    "<br>**Military rank:** Second Lieutenant, First Lieutenant, Captain, Major, Lieutenant Colonel, Colonel, etc.\n",
    "<br>**Clothing size:** Small, Medium, Large, Extra Large. Etc.\n",
    "<br>**Class rank in an exam:** 1,2,3,4,5, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interval Scale of Measurement\n",
    "If the differences between values have meanings, the data are measured at the interval scale. \n",
    "<br>**Temperature:** 10, 20, 30, 40, etc.\n",
    "<br>**IQ rating:** 85 - 114, 115 - 129, 130 - 144, 145 – 159, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio Scale of Measurement\n",
    "Data measured on a ratio scale have differences that are meaningful, and relate to some\n",
    "true zero point. This is the most common scale of measurement. <br>\n",
    "**Weight:** 10, 20, 30, 40, 50, 60, etc.<br>\n",
    "**Height:** 5, 6, 7, 8, 9, etc.<br>\n",
    "**Age:** 1, 2, 3, 4, 5, 6, 7, etc.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Missing Data\n",
    "Missing data can mislead or create problems for analyzing the data. In order to avoid any such issues, you need to impute missing data. There are four most commonly used techniques for data imputation.<br>\n",
    "- **Delete:** You could simply delete the rows containing missing values. This technique is more suitable and effective when the number of missing value rows count is insignificant **(say < 5%)** compare to the overall record count. You can achieve this using Panda's dropna() function.<br>\n",
    "- **Replace** with summary such as  the mean, mode, or median for a respective column. For continuous or quantitative variables, either mean/average or mode or median value of the respective column can be used to replace the missing values. Whereas for categorical or qualitative variables, the mode (most frequent) summation technique works better. You can achieve this using Panda's fillna() function.<br>\n",
    "- **Random replace**: You can also replace the missing values with a randomly picked value from the respective column. This technique would be appropriate where the missing values row count is insignificant.<br>\n",
    "- **Using predictive model:** This is an advanced technique. Here  you can train a regression model for continuous variables and classification model for categorical variables with the available data and use the model to predict the missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Categorical Data\n",
    "Most of the machine’s learning libraries are designed to work well with numerical variables. So categorical variables in their original form of text description can’t be  directly used for model building. Let’s learn some of the common methods of handling categorical data based on their number of levels. <br>\n",
    "- **Create dummy variable:** This is a Boolean variable that indicates the presence of a category with the value 1 and 0 for absence. You should create k-1 dummy variables, where k is the number of levels. Scikit-learn provides a useful function ‘One Hot Encoder’ to create a dummy variable for a given categorical variable.\n",
    "- **Convert to number:** Another simple method is to represent the text description of each level with a number by using the ‘Label Encoder’ function of Scikit-learn. If the number of levels are high (example zip code, state, etc.), then you apply the business logic to combine levels to groups. For example zip code or state can be combined to regions; however, in this method there is a risk of losing critical information. Another method is to combine categories based on similar frequency (new category can be high, medium, low)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        A   B\n",
      "0    high  10\n",
      "1  medium  20\n",
      "2     low  30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "df = pd.DataFrame({'A': ['high', 'medium', 'low'],\n",
    "                   'B': [10,20,30]},\n",
    "                    index=[0, 1, 2])\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    B  A_high  A_low  A_medium\n",
      "0  10       1      0         0\n",
      "1  20       0      0         1\n",
      "2  30       0      1         0\n"
     ]
    }
   ],
   "source": [
    "# using get_dummies function of pandas package\n",
    "df_with_dummies= pd.get_dummies(df, prefix='A', columns=['A'])\n",
    "print (df_with_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting categorical variable to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        A   B  A_pd_factorized  A_LabelEncoded\n",
      "0    high  10                0               0\n",
      "1  medium  20                1               2\n",
      "2     low  30                2               1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# using pandas package's factorize function\n",
    "df['A_pd_factorized'] = pd.factorize(df['A'])[0]\n",
    "# Alternatively you can use sklearn package's LabelEncoder function\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['A_LabelEncoded'] = le.fit_transform(df.A)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Data\n",
    "A unit or scale of measurement for different variables varies, so an analysis with the raw measurement could be artificially skewed toward the variables with higher absolute values. Bringing all the different types of variable units in the same order of magnitude thus eliminates the potential outlier measurements that would misrepresent the finding\n",
    "and negatively affect the accuracy of the conclusion. Two broadly used methods for rescaling data are **normalization** and **standardization.**\n",
    "<br><br>\n",
    "- **Normalizing data**  can be achieved by Min-Max scaling; the formula is given below, which will scale all numeric values in the range 0 to 1.\n",
    "![alt text][logo]\n",
    "\n",
    "[logo]: https://github.com/sara-kassani/Python/blob/master/data/Normalization.JPG?raw=true \"Normalization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note**   \n",
    "Ensure you remove extreme outliers before applying the above technique as it\n",
    "can skew the normal values in your data to a small interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standardization technique will transform the variables to have a zero mean and standard deviation of one. The formula for standardization is given below and the outcome is commonly known as z-scores.\n",
    "\n",
    "![alt text][logo]\n",
    "\n",
    "[logo]: https://github.com/sara-kassani/Python/blob/master/data/Standardization.JPG?raw=true \"Normalization\"\n",
    "\n",
    "Where μ is the mean and σ is the standard deviation. Standardization has often been the preferred method for various analysis as it tells us where each data point lies within its distribution and a rough indication of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean before standardization: petal length=3.8, petal width=1.2\n",
      "SD before standardization: petal length=1.8, petal width=0.8\n",
      "Mean after standardization: petal length=0.0, petal width=-0.0\n",
      "SD after standardization: petal length=1.0, petal width=1.0\n",
      "Min value before min-max scaling: patel length=1.0, patel width=0.1\n",
      "Max value before min-max scaling: petal length=6.9, petal width=2.5\n",
      "Min value after min-max scaling: patel length=0.0, patel width=0.0\n",
      "Max value after min-max scaling: petal length=1.0, petal width=1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalization and scaling\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_std = std_scale.transform(X)\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(X)\n",
    "X_minmax = minmax_scale.transform(X)\n",
    "print (('Mean before standardization: petal length={:.1f}, petal width={:.1f}'.format(X[:,0].mean(), X[:,1].mean())))\n",
    "print (('SD before standardization: petal length={:.1f}, petal width={:.1f}'.format(X[:,0].std(), X[:,1].std())))\n",
    "print (('Mean after standardization: petal length={:.1f}, petal width={:.1f}'.format(X_std[:,0].mean(), X_std[:,1].mean())))\n",
    "print (('SD after standardization: petal length={:.1f}, petal width={:.1f}'.format(X_std[:,0].std(), X_std[:,1].std())))\n",
    "print (('Min value before min-max scaling: patel length={:.1f}, patel width={:.1f}'.format(X[:,0].min(), X[:,1].min())))\n",
    "print (('Max value before min-max scaling: petal length={:.1f}, petal width={:.1f}'.format(X[:,0].max(), X[:,1].max())))\n",
    "print (('Min value after min-max scaling: patel length={:.1f}, patel width={:.1f}'.format(X_minmax[:,0].min(), X_minmax[:,1].min())))\n",
    "print (('Max value after min-max scaling: petal length={:.1f}, petal width={:.1f}'.format(X_minmax[:,0].max(), X_minmax[:,1].max())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
