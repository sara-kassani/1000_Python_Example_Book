{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Tutorial: scikit-learn new features</h1>\n",
    "\n",
    "<br />\n",
    "<div align=\"center\">April 28, 2019</div>\n",
    "<br />\n",
    "\n",
    "<div align=\"center\">Roman Yurchak (notebook adapted from work by Olivier Grisel)</div>\n",
    "\n",
    "Running this notebooks requires Python 3.5+ as well as,\n",
    " - scikit-learn >=0.21.2\n",
    " - matplotlib\n",
    " - pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example: the California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "\n",
    "calhousing = fetch_california_housing()\n",
    "\n",
    "X = pd.DataFrame(calhousing.data, columns=calhousing.feature_names)\n",
    "y = pd.Series(calhousing.target, name='house_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calhousing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.head().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spatial clustering\n",
    "\n",
    "Let's looks at the spatial distribution of census block groups, and the corresponding median house value,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "X.plot.scatter('Longitude', 'Latitude', c=y, ax=ax,\n",
    "               colorbar=True, colormap='Reds', alpha=0.7, s=10)\n",
    "ax.set_title(y.name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we try to detect geographical entities (towns, cities) from this spatial distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.1).fit(X[['Longitude', 'Latitude']])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "labels = dbscan.labels_\n",
    "labels[labels>=0] += 5\n",
    "print('n_clusters:', len(np.unique(labels)))\n",
    "\n",
    "X.plot.scatter('Longitude', 'Latitude', c=labels, ax=ax,\n",
    "               colorbar=True, colormap='viridis', alpha=0.7, s=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples that don't belong to any cluster (noisy samples) have a value of -1.\n",
    "\n",
    "The choice of `eps` parameter will strongly impact both the number of clusters and the fraction of noisy samples in DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "cl = OPTICS().fit(X[['Longitude', 'Latitude']])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "labels = cl.labels_\n",
    "labels[labels>=0] += 5\n",
    "print('n_clusters:', len(np.unique(labels)))\n",
    "\n",
    "X.plot.scatter('Longitude', 'Latitude', c=labels, ax=ax,\n",
    "               colorbar=True, colormap='viridis', alpha=0.7, s=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline supervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a quick baseline model: linear regression (aka. Ordinary Least Squares):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%time lm = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train error: %0.3f, test error: %0.3f\" %\n",
    "      (median_absolute_error(y_train, lm.predict(X_train)),\n",
    "       median_absolute_error(y_test, lm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_predictions(y_pred, y_true):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('true target')\n",
    "    plt.xlim(-1, 6)\n",
    "    plt.ylim(-1, 6)\n",
    "    plt.scatter(y_pred, y_true)\n",
    "    \n",
    "scatter_predictions(lm.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty bad: the errors (off-diagonal predictions) seems to be heteroschedastic and there is a saturation effect with many samples with `y_true == 5`. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y.hist(bins=50)\n",
    "\n",
    "ax.set_xlabel(y.name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter out the \"anomalies\" and make the target variable marginal distribution more \"Gaussian\" by taking the log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(y[y<4.9]).hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X[y<4.9], y[y<4.9], test_size=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "lm2 = make_pipeline(StandardScaler(), LinearRegression())\n",
    "_ = %time lm2.fit(X_train, np.log(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train error: %0.3f, test error: %0.3f\" %\n",
    "      (median_absolute_error(y_train,\n",
    "                             np.exp(lm2.predict(X_train))),\n",
    "       median_absolute_error(y_test,\n",
    "                             np.exp(lm2.predict(X_test)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter_predictions(\n",
    "    np.exp(lm2.predict(X_test)),\n",
    "    y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering / preprocessing\n",
    "\n",
    "To facilitate the evaluation of subsequent models, let's factorize the evaluation code into a separate function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dumps\n",
    "\n",
    "def train_score_model(\n",
    "        model, X_train, X_test, y_train, y_test, plot=False\n",
    "):\n",
    "    %time model.fit(X_train, np.log(y_train))\n",
    "\n",
    "    print(\"train error: %0.3f, test error: %0.3f\" %\n",
    "          (median_absolute_error(\n",
    "              y_train, np.exp(model.predict(X_train))),\n",
    "           median_absolute_error(\n",
    "              y_test,  np.exp(model.predict(X_test)))))\n",
    "    \n",
    "    model_size = len(dumps(model)) / 1e6\n",
    "    if model_size > 0.1:\n",
    "        print(\"Model size: %0.2f MB\" % (model_size))\n",
    "\n",
    "    if plot:\n",
    "        scatter_predictions(\n",
    "            np.exp(model.predict(X_test)), y_test\n",
    "        )\n",
    "    return model\n",
    "\n",
    "train_score_model(lm2, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine more closely the distribution of different features. Most are assymetric distributions with a long tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(12, 8))\n",
    "X.hist(ax=ax, bins=50);\n",
    "ax[0,0].set_xlim(0, 10)\n",
    "ax[0,1].set_xlim(0, 200)\n",
    "ax[0,2].set_xlim(0, 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import make_union\n",
    "\n",
    "plm = make_pipeline(\n",
    "    ColumnTransformer([\n",
    "        ('scaler', StandardScaler(),\n",
    "         ['Latitude', 'Longitude', \"HouseAge\"]),\n",
    "        ('power_transform', PowerTransformer(),\n",
    "         ['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup'])\n",
    "    ]),\n",
    "    LinearRegression())\n",
    "\n",
    "train_score_model(plm, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `ColumnsTransformer` to selectively apply Yeo-Johnson power transform on some features, while keeping `StandardScaler` for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_union\n",
    "\n",
    "plm = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    make_union(\n",
    "        FunctionTransformer(validate=True),\n",
    "        PolynomialFeatures(degree=3)\n",
    "    ),\n",
    "    LinearRegression())\n",
    "\n",
    "train_score_model(plm, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model improvement\n",
    "\n",
    "In this part, we keep the baseline feature scaling, while evaluating more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(hidden_layer_sizes=(100, 10, 10), activation='relu'),\n",
    ")\n",
    "\n",
    "train_score_model(mlp, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(learning_rate=0.1, max_depth=8, min_samples_leaf=20,\n",
    "                                n_estimators=100, loss='huber')\n",
    "\n",
    "train_score_model(gbr, X_train, X_test, y_train, y_test, plot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gbr.predict(X_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "train_score_model(rfr, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit rfr.predict(X_test[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Histogram-based Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr = HistGradientBoostingRegressor(min_samples_leaf=20, max_leaf_nodes=256,\n",
    "                                     max_iter=100)\n",
    "train_score_model(hgbr, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hgbr.predict(X_test[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr = HistGradientBoostingRegressor(\n",
    "    min_samples_leaf=20, max_leaf_nodes=256,\n",
    "    n_iter_no_change=5, validation_fraction=0.1,\n",
    "    scoring=\"loss\", max_iter=10000\n",
    ")\n",
    "\n",
    "train_score_model(hgbr, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(hgbr.train_score_, label=\"train\")\n",
    "ax.plot(hgbr.validation_score_, \"--\", label=\"validation\")\n",
    "ax.set_xlabel(\"Boosting iteration (trees)\")\n",
    "ax.set_ylabel(\"Negative loss (MSE)\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Imputation\n",
    "\n",
    "Let's artificially create some missing data so that we can illustrate the imputation estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "density = 4  # one in 10 values will be NaN\n",
    "\n",
    "mask = rng.randint(density, size=X.shape) == 0\n",
    "X_na = X.copy()\n",
    "X_na.values[mask] = np.nan\n",
    "X_na.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_na, X_test_na, y_train_na, y_test_na = train_test_split(\n",
    "    X_na[y<4.9], y[y<4.9], test_size=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    LinearRegression()\n",
    ")\n",
    "print('Baseline model\\n')\n",
    "train_score_model(model, X_train, X_test, y_train, y_test);\n",
    "print('\\nBaseline model with missing data + SimpleImputer\\n')\n",
    "train_score_model(model, X_train_na, X_test_na, y_train_na, y_test_na);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    IterativeImputer(),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "train_score_model(pipe, X_train_na, X_test_na, y_train_na, y_test_na);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `IterativeImputer` we almost fully compensate the existence of missing values with respect to the baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm started models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small, X_val, y_train_small, y_val = train_test_split(\n",
    "    X_train, y_train, train_size=5000, test_size=1000)\n",
    "\n",
    "val_errors = []\n",
    "train_errors = []\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(learning_rate=0.1, max_depth=8,\n",
    "                                min_samples_leaf=3, n_estimators=1)\n",
    "\n",
    "gbr.fit(X_train_small, y_train_small)\n",
    "\n",
    "train_error = median_absolute_error(y_train, gbr.predict(X_train))\n",
    "val_error = median_absolute_error(y_test, gbr.predict(X_test))\n",
    "\n",
    "train_errors.append(train_error)\n",
    "val_errors.append(val_error)\n",
    "\n",
    "print(\"train error: %0.3f, test error: %0.3f\" % (train_error, val_error)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    gbr.set_params(warm_start=True, n_estimators=len(gbr.estimators_) + 1)\n",
    "    gbr.fit(X_train_small, y_train_small)\n",
    "    train_error = median_absolute_error(y_train, gbr.predict(X_train))\n",
    "    val_error = median_absolute_error(y_test, gbr.predict(X_test))\n",
    "\n",
    "    train_errors.append(train_error)\n",
    "    val_errors.append(val_error)\n",
    "    if (i + 2) % 10 == 0:\n",
    "        print(\"n_trees=%d, train error: %0.3f, test error: %0.3f\"\n",
    "              % (len(gbr.estimators_), train_error, val_error)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_indices = np.arange(len(val_errors)) + 1\n",
    "plt.plot(tree_indices, train_errors, label='training')\n",
    "plt.plot(tree_indices, val_errors, label='validation')\n",
    "plt.xlabel('number of trees')\n",
    "plt.ylabel('mean absolute error')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features encoded as integers\n",
    "\n",
    "One-hot encoding is pretty useless for tree-based models (at least in scikit-learn). Contrary to other models it's pretty safe and much more efficient to use integer based encoding for instance using pandas:\n",
    "\n",
    "```python\n",
    "    >>> categorical_data.apply(lambda x: pd.factorize(x)[0])\n",
    "```\n",
    "\n",
    "alternatively you can use scikit-learn's [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
